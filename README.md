This repo enables active data collection to add during training, built off the amazing work done by the [nerfstudio](https://docs.nerf.studio/) team, and is the codebase for `Data Augmentation for NeRFs in the Low Data Limit` [submitted to ICRA '25].

# Quickstart
### Install
Follow the installation procedure from Nerfstudio; our code was built off commit [516fd7c](https://github.com/nerfstudio-project/nerfstudio/commit/516fd7c9ac73e28db6a522df57d7b22e7b0d6756). We run our code on a server with Ubuntu 20.04 and an Nvidia RTX 6000 Ada card.

To summarize:
```python
git clone git@github.com:Agaggar/nerfstudio-low-data.git
cd nerfstudio
pip install --upgrade pip setuptools
pip install -e .
```

### Data
You'll also need some data to train your NeRF model with. We provide sample files (from the lego dozer, scaled down 0.1x) in data/lego_scale

The easiest way to get sample data is to download the [Blender dataset](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1) from the original NeRF paper (nerf_synthetic.zip). You can then follow the Nerfstudio guide to training with Blender data [here](https://docs.nerf.studio/quickstart/existing_dataset.html).

Alternatively, you can record a video in real life and use COLMAP to label poses with images (following [this guide](https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md#colmap) by Nvidia's amazing instant NGP team).

### Blender
If you want to augment additional images in Blender, download [Blender software](https://www.blender.org/); we run Blender v4.1.1 on Linux. The blend files we use are located `data/blender_files`, and are scaled down by 0.1x.

# Using our code
Running `ns-train nerfacto-active --data PATH_TO_DATA --random-seed POSITIVE_INTEGER` sets the default configuration for active, iterative training.

For example:
```python
ns-train nerfacto-active --data data/lego_scale --random-seed 0 --subset-data 10 --max-data 15 --add-amount 5 --data-selector nbb
```

Additional flags:
- `--iterative_training: bool = False`: Whether to train iteratively (add data during training)
- `--steps_per_iterative_add`: Number of steps between adding data. i.e., 200 means add `add_amount` data every 200 iterations until `max-data` reached.
- `--subset_data`: How many images to start training with
- `--random_seed`: Random seed for sampling data from total training set. Must be specified for data augmenting.
- `--max_data`: max number of data to add to in training set
- `--add_amount`: how many images to add between each training step
- `--data_selector`: if iterative training, method for adding data. currently supports uniform and our method (nbb)

## Citing
If you find this code useful for your research or the use data generated by our method, please consider citing our (after release).
